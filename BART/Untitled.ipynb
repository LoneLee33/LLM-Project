{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52be09c5-2740-43d5-a104-5aaeec1f410a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22c5f1b3-bd90-42bf-9d49-248293c3f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "data_dir = r\"D:\\LLM\\DATA\"\n",
    "\n",
    "def read_ner_file(filename):\n",
    "    path = os.path.join(data_dir, filename)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw = f.read().strip().split(\"-DOCSTART-\")[1:]\n",
    "    examples = []\n",
    "    for block in raw:\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        tokens, labels = [], []\n",
    "        for line in lines:\n",
    "            if line.strip() == \"\":\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:\n",
    "                token, label = parts\n",
    "            elif len(parts) == 3:\n",
    "                token, _, label = parts\n",
    "            else:\n",
    "                continue  # skip malformed lines\n",
    "            tokens.append(token)\n",
    "            labels.append(label)\n",
    "        if tokens and labels:\n",
    "            examples.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return examples\n",
    "\n",
    "ner_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(read_ner_file(\"train.txt\")),\n",
    "    \"validation\": Dataset.from_list(read_ner_file(\"dev.txt\")),\n",
    "    \"test\": Dataset.from_list(read_ner_file(\"test.txt\")),\n",
    "})\n",
    "\n",
    "label_list = sorted(set(label for ex in ner_dataset[\"train\"] for label in ex[\"ner_tags\"]))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(label_to_id[label[word_idx]] if label[word_idx].startswith(\"I-\") else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70b23603-5bea-4b9f-849b-02753acd182e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Error processing line: 'x'\n",
      "Converted file saved to: D:\\LLM\\DATA\\train_bart_ready_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "input_path = Path(r\"D:\\LLM\\DATA\\test.jsonl\")\n",
    "output_path = Path(r\"D:\\LLM\\DATA\\test_bart_ready.jsonl\")\n",
    "\n",
    "# Tags to wrap from spans\n",
    "TAGS = {\n",
    "    \"VAR\": \"var\",\n",
    "    \"PARAM\": \"param\",\n",
    "    \"OBJ_NAME\": \"obj_name\",\n",
    "    \"CONST_DIR\": \"const_dir\",\n",
    "    \"LIMIT\": \"limit\",\n",
    "    \"OBJ_DIR\": \"obj_dir\"\n",
    "}\n",
    "\n",
    "# Replace spaces in variable names to keep LP output valid\n",
    "def sanitize(varname):\n",
    "    return varname.replace(\" \", \"_\")\n",
    "\n",
    "# Wrap XML tags around the span-marked words\n",
    "def wrap_spans(text, spans):\n",
    "    spans = sorted(spans, key=lambda x: x[\"start\"])  # Sort by position\n",
    "    wrapped = \"\"\n",
    "    last_idx = 0\n",
    "    for span in spans:\n",
    "        start, end = span[\"start\"], span[\"end\"]\n",
    "        label = span[\"label\"]\n",
    "        tag = TAGS.get(label)\n",
    "        if not tag:\n",
    "            continue\n",
    "        wrapped += text[last_idx:start]\n",
    "        wrapped += f\"<{tag}>{text[start:end]}</{tag}>\"\n",
    "        last_idx = end\n",
    "    wrapped += text[last_idx:]\n",
    "    return wrapped\n",
    "\n",
    "# Convert one entry from original format to {\"input\": ..., \"output\": ...}\n",
    "def convert_entry(entry):\n",
    "    entry = next(iter(entry.values()))  # unwrap hash-keyed entry\n",
    "    raw_text = entry[\"document\"]\n",
    "    spans = entry.get(\"spans\", [])\n",
    "    tagged_input = wrap_spans(raw_text, spans)\n",
    "\n",
    "    # LP objective\n",
    "    obj = entry[\"obj_declaration\"]\n",
    "    obj_str = f\"{obj['direction'][:3]}: \"\n",
    "    obj_terms = [f\"{coef} {sanitize(var)}\" for var, coef in obj.get(\"terms\", {}).items()]\n",
    "    obj_str += \" + \".join(obj_terms)\n",
    "\n",
    "    # Constraints\n",
    "    const_strs = []\n",
    "    for c in entry.get(\"const_declarations\", []):\n",
    "        if c[\"type\"] == \"sum\":\n",
    "            const_strs.append(f\"{' + '.join(sanitize(v) for v in obj['terms'].keys())} <= {c['limit']}\")\n",
    "        elif c[\"type\"] == \"lowerbound\":\n",
    "            const_strs.append(f\"{sanitize(c['var'])} >= {c['limit']}\")\n",
    "        elif c[\"type\"] == \"upperbound\":\n",
    "            const_strs.append(f\"{sanitize(c['var'])} <= {c['limit']}\")\n",
    "\n",
    "    output = obj_str + \"\\nst: \" + \"\\n\".join(const_strs)\n",
    "\n",
    "    return {\n",
    "        \"input\": tagged_input.strip(),\n",
    "        \"output\": output.strip()\n",
    "    }\n",
    "\n",
    "# Process all lines\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        try:\n",
    "            converted = convert_entry(data)\n",
    "            fout.write(json.dumps(converted) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(\"Error processing line:\", e)\n",
    "\n",
    "print(\"Converted file saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5196cbba-addb-4c36-8f07-4b42648f3e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a643f9bb9eb420c8f5106a03bef4f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/713 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60892a20cb324514b0a221ab54775238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/98 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86633d0073f54c9fa517a212c7cdce9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/289 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lonel\\AppData\\Local\\Temp\\ipykernel_66508\\348126509.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1790' max='1790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1790/1790 3:49:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.012200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.011600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1790, training_loss=0.2572715645395844, metrics={'train_runtime': 13792.0482, 'train_samples_per_second': 0.517, 'train_steps_per_second': 0.13, 'total_flos': 7725727906529280.0, 'train_loss': 0.2572715645395844, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# posttrain_bart_lp.py\n",
    "import os\n",
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = r\"D:\\LLM\\DATA\"\n",
    "train_file = os.path.join(DATA_DIR, \"train_bart_ready.jsonl\")\n",
    "dev_file = os.path.join(DATA_DIR, \"dev_bart_ready.jsonl\")\n",
    "test_file = os.path.join(DATA_DIR, \"test_bart_ready.jsonl\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(load_jsonl(train_file)),\n",
    "    \"validation\": Dataset.from_list(load_jsonl(dev_file)),\n",
    "    \"test\": Dataset.from_list(load_jsonl(test_file)),\n",
    "})\n",
    "\n",
    "# Load BART tokenizer & model\n",
    "checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "\n",
    "# Preprocess (tokenization)\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(example[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label_enc = tokenizer(example[\"output\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "    input_enc[\"labels\"] = label_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_lp_model\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    do_eval=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2de73754-c2cc-4053-a4b2-94a9561c89a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bart_lp_model\\\\tokenizer_config.json',\n",
       " './bart_lp_model\\\\special_tokens_map.json',\n",
       " './bart_lp_model\\\\vocab.json',\n",
       " './bart_lp_model\\\\merges.txt',\n",
       " './bart_lp_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./bart_lp_model\")\n",
    "tokenizer.save_pretrained(\"./bart_lp_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f37358d-bc3b-4e22-b5e9-48401e42df09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.027730919420719147, 'eval_runtime': 12.8924, 'eval_samples_per_second': 22.416, 'eval_steps_per_second': 5.662, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset=tokenized_dataset[\"test\"])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ba93b5-e0a6-4564-8314-23a5b3955b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in model folder: [WindowsPath('D:/LLM/BART/bart_lp_model/checkpoint-1500'), WindowsPath('D:/LLM/BART/bart_lp_model/checkpoint-1790'), WindowsPath('D:/LLM/BART/bart_lp_model/config.json'), WindowsPath('D:/LLM/BART/bart_lp_model/generation_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model/merges.txt'), WindowsPath('D:/LLM/BART/bart_lp_model/model.safetensors'), WindowsPath('D:/LLM/BART/bart_lp_model/special_tokens_map.json'), WindowsPath('D:/LLM/BART/bart_lp_model/tokenizer_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model/vocab.json')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Use pure local Path object without slashes\n",
    "model_dir = Path(\"D:/LLM/BART/bart_lp_model\").resolve()\n",
    "\n",
    "# 2. Confirm required files exist\n",
    "print(\"Files in model folder:\", list(model_dir.glob(\"*\")))\n",
    "\n",
    "# 3. Load model + tokenizer safely\n",
    "tokenizer = BartTokenizer.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595445f8-6e77-4dd7-98b0-76547c1551c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " A grocery store wants to liquidate its <const_dir>stock</const_dir> of <limit>10</limit> apples, <limit>20</limit> bananas, and <limit>80</limit> grapes. Given past experience, the store knows that they can propose a <var>banana-haters package</var> with <param>6</param> apples and <param>30</param> grapes and that this package will bring a <obj_name>profit</obj_name> of <param>six</param> euros. Similarly, they can prepare a <var>combo package</var> with <param>5</param> apples, <param>6</param> bananas, and <param>20</param> grapes, yielding a <obj_name>profit</obj_name> of <param>seven</param> euros. They know they can sell any quantity of these two packages within the availability of its stock. What quantity of each package, <var>banana-haters packages</var> and <var>combo packages</var>, should the store prepare to <obj_dir>maximize</obj_dir> <obj_name>net profit</obj_name>?\n",
      "Output:\n",
      " max: six banana-haters_package + seven combo_package\n",
      "st:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_lp(statement: str, max_len=256):\n",
    "    inputs = tokenizer(statement, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_len, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a tagged statement:\n",
    "tagged_input = (\n",
    "    \"A grocery store wants to liquidate its <const_dir>stock</const_dir> of <limit>10</limit> apples, <limit>20</limit> bananas, and <limit>80</limit> grapes. Given past experience, the store knows that they can propose a <var>banana-haters package</var> with <param>6</param> apples and <param>30</param> grapes and that this package will bring a <obj_name>profit</obj_name> of <param>six</param> euros. Similarly, they can prepare a <var>combo package</var> with <param>5</param> apples, <param>6</param> bananas, and <param>20</param> grapes, yielding a <obj_name>profit</obj_name> of <param>seven</param> euros. They know they can sell any quantity of these two packages within the availability of its stock. What quantity of each package, <var>banana-haters packages</var> and <var>combo packages</var>, should the store prepare to <obj_dir>maximize</obj_dir> <obj_name>net profit</obj_name>?\"\n",
    ")\n",
    "\n",
    "print(\"Input:\\n\", tagged_input)\n",
    "print(\"Output:\\n\", generate_lp(tagged_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31634d2a-199f-4b5d-8977-d22caaf0c1fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Error processing entry: 'factor'\n",
      "Error processing entry: 'x_var'\n",
      "Converted file saved to:\n",
      "D:\\LLM\\DATA\\train_bart_ready_1_bart.jsonl\n"
     ]
    }
   ],
   "source": [
    "# New conversion\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "input_path = Path(r\"D:\\LLM\\DATA\\train.jsonl\")\n",
    "output_path = Path(r\"D:\\LLM\\DATA\\train_bart_ready_1_bart.jsonl\")\n",
    "\n",
    "# Tags to wrap from spans\n",
    "TAGS = {\n",
    "    \"VAR\": \"var\",\n",
    "    \"PARAM\": \"param\",\n",
    "    \"OBJ_NAME\": \"obj_name\",\n",
    "    \"CONST_DIR\": \"const_dir\",\n",
    "    \"LIMIT\": \"limit\",\n",
    "    \"OBJ_DIR\": \"obj_dir\"\n",
    "}\n",
    "\n",
    "def sanitize(varname):\n",
    "    return varname.replace(\" \", \"_\")\n",
    "\n",
    "def wrap_spans(text, spans):\n",
    "    spans = sorted(spans, key=lambda x: x[\"start\"])\n",
    "    wrapped = \"\"\n",
    "    last_idx = 0\n",
    "    for span in spans:\n",
    "        start, end = span[\"start\"], span[\"end\"]\n",
    "        label = span[\"label\"]\n",
    "        tag = TAGS.get(label)\n",
    "        if not tag:\n",
    "            continue\n",
    "        wrapped += text[last_idx:start]\n",
    "        wrapped += f\"<{tag}>{text[start:end]}</{tag}>\"\n",
    "        last_idx = end\n",
    "    wrapped += text[last_idx:]\n",
    "    return wrapped\n",
    "\n",
    "def convert_entry(entry):\n",
    "    entry = next(iter(entry.values()))\n",
    "    raw_text = entry[\"document\"]\n",
    "    spans = entry.get(\"spans\", [])\n",
    "    tagged_input = wrap_spans(raw_text, spans)\n",
    "\n",
    "    # Objective\n",
    "    obj = entry[\"obj_declaration\"]\n",
    "    obj_str = f\"{obj['direction'][:3]}: \"\n",
    "    obj_terms = [f\"{coef} {sanitize(var)}\" for var, coef in obj.get(\"terms\", {}).items()]\n",
    "    obj_str += \" + \".join(obj_terms)\n",
    "\n",
    "    # Constraints\n",
    "    const_strs = []\n",
    "    for c in entry.get(\"const_declarations\", []):\n",
    "        ctype = c.get(\"type\")\n",
    "\n",
    "        if ctype == \"sum\":\n",
    "            const_strs.append(f\"{' + '.join(sanitize(v) for v in obj['terms'].keys())} <= {c['limit']}\")\n",
    "\n",
    "        elif ctype == \"lowerbound\":\n",
    "            const_strs.append(f\"{sanitize(c['var'])} >= {c['limit']}\")\n",
    "\n",
    "        elif ctype == \"upperbound\":\n",
    "            const_strs.append(f\"{sanitize(c['var'])} <= {c['limit']}\")\n",
    "\n",
    "        elif ctype == \"linear\":\n",
    "            terms = [f\"{coef} {sanitize(var)}\" for var, coef in c.get(\"terms\", {}).items()]\n",
    "            operator = {\n",
    "                \"LESS_OR_EQUAL\": \"<=\",\n",
    "                \"GREATER_OR_EQUAL\": \">=\",\n",
    "                \"EQUAL\": \"=\"\n",
    "            }.get(c.get(\"operator\", \"\"), \"<=\")\n",
    "            const_strs.append(f\"{' + '.join(terms)} {operator} {c['limit']}\")\n",
    "\n",
    "        elif ctype == \"xby\":\n",
    "            x = sanitize(c[\"x_var\"])\n",
    "            y = sanitize(c[\"y_var\"])\n",
    "            const_strs.append(f\"{x} >= {c['factor']} {y}\")\n",
    "\n",
    "        elif ctype == \"ratio\":\n",
    "            x = sanitize(c[\"x_var\"])\n",
    "            y = sanitize(c[\"y_var\"])\n",
    "            direction = c.get(\"direction\", \"\").lower()\n",
    "            op = {\n",
    "                \"less than\": \"<=\",\n",
    "                \"greater than\": \">=\",\n",
    "                \"equal to\": \"=\"\n",
    "            }.get(direction, \"<=\")\n",
    "            const_strs.append(f\"{x} {op} {c['ratio']} {y}\")\n",
    "\n",
    "        elif ctype == \"xy\":\n",
    "            x = sanitize(c[\"x_var\"])\n",
    "            y = sanitize(c[\"y_var\"])\n",
    "            direction = c.get(\"direction\", \"\").lower()\n",
    "            op = {\n",
    "                \"less than\": \"<=\",\n",
    "                \"greater than\": \">=\",\n",
    "                \"equal to\": \"=\"\n",
    "            }.get(direction, \"<=\")\n",
    "            const_strs.append(f\"{x} {op} {y}\")\n",
    "\n",
    "    output = obj_str\n",
    "    if const_strs:\n",
    "        output += \"\\nst: \" + \"\\n     \".join(const_strs)\n",
    "\n",
    "    return {\n",
    "        \"input\": tagged_input.strip(),\n",
    "        \"output\": output.strip()\n",
    "    }\n",
    "\n",
    "# Run conversion\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as fin, open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        data = json.loads(line)\n",
    "        try:\n",
    "            converted = convert_entry(data)\n",
    "            fout.write(json.dumps(converted) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(\"Error processing entry:\", e)\n",
    "\n",
    "print(f\"Converted file saved to:\\n{output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60659872-fbac-40cf-85d3-b71546398088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb333b376898409e99fc244a6216de8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/521 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fdb185d5d94b7ba9431420457f8785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ba6d5523764b3a8fdbbe2104b06722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\lonel\\AppData\\Local\\Temp\\ipykernel_37616\\2322109450.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1310' max='1310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1310/1310 08:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.009400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1310, training_loss=0.36783181967796713, metrics={'train_runtime': 493.0485, 'train_samples_per_second': 10.567, 'train_steps_per_second': 2.657, 'total_flos': 5645307488501760.0, 'train_loss': 0.36783181967796713, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# posttrain_bart_lp.py\n",
    "import os\n",
    "import json\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = r\"D:\\LLM\\DATA\"\n",
    "train_file = os.path.join(DATA_DIR, \"train_bart_ready_1_bart.jsonl\")\n",
    "dev_file = os.path.join(DATA_DIR, \"dev_bart_ready_1.jsonl\")\n",
    "test_file = os.path.join(DATA_DIR, \"test_bart_ready_1.jsonl\")\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(load_jsonl(train_file)),\n",
    "    \"validation\": Dataset.from_list(load_jsonl(dev_file)),\n",
    "    \"test\": Dataset.from_list(load_jsonl(test_file)),\n",
    "})\n",
    "\n",
    "# Load BART tokenizer & model\n",
    "checkpoint = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "\n",
    "# Preprocess (tokenization)\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(example[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label_enc = tokenizer(example[\"output\"], max_length=256, truncation=True, padding=\"max_length\")\n",
    "    input_enc[\"labels\"] = label_enc[\"input_ids\"]\n",
    "    return input_enc\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_lp_model\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,  # safe with 16GB + fp16\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    do_eval=True,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # Recommended for faster and more memory-efficient training\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cff959c1-2581-4a65-b5c5-bc154c743b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./bart_lp_model_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./bart_lp_model_2\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\modeling_utils.py:3564\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3559\u001b[39m     gc.collect()\n\u001b[32m   3561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3562\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3563\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3564\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3565\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3566\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./bart_lp_model_2\")\n",
    "tokenizer.save_pretrained(\"./bart_lp_model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ca84b35-1da2-462a-b0ff-2062023f108f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in model folder: [WindowsPath('D:/LLM/BART/bart_lp_model_2/config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/generation_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/merges.txt'), WindowsPath('D:/LLM/BART/bart_lp_model_2/model.safetensors'), WindowsPath('D:/LLM/BART/bart_lp_model_2/special_tokens_map.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/tokenizer_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/vocab.json')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Use pure local Path object without slashes\n",
    "model_dir = Path(\"D:/LLM/BART/bart_lp_model_2\").resolve()\n",
    "\n",
    "# 2. Confirm required files exist\n",
    "print(\"Files in model folder:\", list(model_dir.glob(\"*\")))\n",
    "\n",
    "# 3. Load model + tokenizer safely\n",
    "tokenizer = BartTokenizer.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1686e0ed-55db-48d1-820f-9899fe4cb1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " A juice bar sells <var>apple juice</var> and <var>orange juice.</var> They can make <const_dir>at most</const_dir> <limit>120</limit> <var>apple juices</var> and <limit>90</limit> <var>orange juices</var> each day. For customer satisfaction, they must sell <const_dir>at least</const_dir> <limit>40</limit> <var>apple juices</var> and <limit>30</limit> <var>orange juices.</var> Each <var>apple juice</var> <obj_name>earns</obj_name> $3 and each <var>orange juice</var> <obj_name>earns</obj_name> $4. How many of each should they sell to <obj_dir>maximize</obj_dir> <obj_name>profit?</obj_name>\n",
      "Output:\n",
      " max: 3 apple_juice + 4.4 orange_jjuice\n",
      "st: apple_juries <= 120\n",
      "    + orange_juices <= 90\n",
      "  + 40\n",
      "  >= 30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_lp(statement: str, max_len=256):\n",
    "    inputs = tokenizer(statement, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_len, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test with a tagged statement:\n",
    "tagged_input = (\n",
    "    \"A juice bar sells <var>apple juice</var> and <var>orange juice.</var> They can make <const_dir>at most</const_dir> <limit>120</limit> <var>apple juices</var> and <limit>90</limit> <var>orange juices</var> each day. For customer satisfaction, they must sell <const_dir>at least</const_dir> <limit>40</limit> <var>apple juices</var> and <limit>30</limit> <var>orange juices.</var> Each <var>apple juice</var> <obj_name>earns</obj_name> $3 and each <var>orange juice</var> <obj_name>earns</obj_name> $4. How many of each should they sell to <obj_dir>maximize</obj_dir> <obj_name>profit?</obj_name>\"\n",
    ")\n",
    "\n",
    "print(\"Input:\\n\", tagged_input)\n",
    "print(\"Output:\\n\", generate_lp(tagged_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b735ebc9-9a5d-42ea-b047-eb46fa5201f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in model folder: [WindowsPath('D:/LLM/BART/bart_lp_model_2/config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/generation_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/merges.txt'), WindowsPath('D:/LLM/BART/bart_lp_model_2/model.safetensors'), WindowsPath('D:/LLM/BART/bart_lp_model_2/special_tokens_map.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/tokenizer_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2/vocab.json')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# 1. Use pure local Path object without slashes\n",
    "model_dir = Path(\"D:/LLM/BART/bart_lp_model_2\").resolve()\n",
    "\n",
    "# 2. Confirm required files exist\n",
    "print(\"Files in model folder:\", list(model_dir.glob(\"*\")))\n",
    "\n",
    "# 3. Load model + tokenizer safely\n",
    "tokenizer = BartTokenizer.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db45922c-5e51-4157-b524-e3debcd728e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e394118f43462e9084b21e8f8d4df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\lonel\\AppData\\Local\\Temp\\ipykernel_75316\\981312486.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 07:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.015147188782691955, metrics={'train_runtime': 461.682, 'train_samples_per_second': 10.83, 'train_steps_per_second': 2.707, 'total_flos': 5417761505280000.0, 'train_loss': 0.015147188782691955, 'epoch': 5.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "# Load new training data\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "new_data_path = Path(\"D:/LLM/DATA/generated_samples_1000.jsonl\")\n",
    "new_dataset = Dataset.from_list(load_jsonl(new_data_path))\n",
    "\n",
    "# Preprocess\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(example[\"output\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = new_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_lp_model_2_with_generated_samples_1000\",\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_continue\",\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29f3f181-7692-435a-8994-83ee79f8dcba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./bart_lp_model_2_with_generated_samples_1000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./bart_lp_model_2_with_generated_samples_1000\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\modeling_utils.py:3564\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   3559\u001b[39m     gc.collect()\n\u001b[32m   3561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   3562\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   3563\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3564\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3565\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3566\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"./bart_lp_model_2_with_generated_samples_1000\")\n",
    "tokenizer.save_pretrained(\"./bart_lp_model_2_with_generated_samples_1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6306fd7b-f646-4121-abf6-bd037ddcc3a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in model folder: [WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/checkpoint-1000'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/checkpoint-1250'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/generation_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/merges.txt'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/model.safetensors'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/special_tokens_map.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/tokenizer_config.json'), WindowsPath('D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000/vocab.json')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# 1. Use pure local Path object without slashes\n",
    "model_dir = Path(\"D:/LLM/BART/bart_lp_model_2_with_generated_samples_1000\").resolve()\n",
    "\n",
    "# 2. Confirm required files exist\n",
    "print(\"Files in model folder:\", list(model_dir.glob(\"*\")))\n",
    "\n",
    "# 3. Load model + tokenizer safely\n",
    "tokenizer = BartTokenizer.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model = BartForConditionalGeneration.from_pretrained(str(model_dir), local_files_only=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9ab3deb-3579-4f79-a66b-80a8690899e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input:\n",
      " A bubble tea store sells <var>peach</var> and <var>mango</var> flavored drinks. The store can make <const_dir>at most</const_dir> <limit>788</limit> drinks in total. To stay in business, they must sell <const_dir>at least</const_dir> <limit>53</limit> <var>mango drinks</var> and <limit>89</limit> <var>peach drinks.</var> However, due to fruit shortages, they can make <const_dir>at most</const_dir> <limit>560</limit> <var>mango drinks</var> and <const_dir>at most</const_dir> <limit>64</limit> <var>peach drinks.</var> The <obj_name>profit</obj_name> per <var>mango drink</var> is <param>$3,</param> and the <obj_name>profit</obj_name> per <var>peach drink</var> is <param>$1.</param> How many of each drink should they sell to <obj_dir>maximize</obj_dir> <obj_name>profit?</obj_name>\n",
      "Output:\n",
      " max: 1 mango_drink + 1 peach_name\n",
      "st: mango_jacket + peach_ink <= 788\n",
      "   _ mango_jackets >= 53\n",
      "  peach_drinks <= 560\n",
      " _   peach-drinks >= 89\n",
      "  <= 64\n"
     ]
    }
   ],
   "source": [
    "# LP generation function\n",
    "def generate_lp(statement: str, max_len=256):\n",
    "    inputs = tokenizer(statement, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=max_len, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Read from file and generate LPs\n",
    "input_file = Path(\"D:/LLM/NER/nl4opt-subtask1-baseline/bart_inputs_single_test.jsonl\")\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        tagged_input = data.get(\"input\", \"\")\n",
    "        if tagged_input:\n",
    "            print(\"\\nInput:\\n\", tagged_input)\n",
    "            print(\"Output:\\n\", generate_lp(tagged_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee688e-bace-4bf3-b36c-1fb1d0a36009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526531b5-14bb-4bc2-910f-9f1d4c17c221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
