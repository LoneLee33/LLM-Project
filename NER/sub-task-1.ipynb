{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb393071-c5e1-49d3-b6a2-db4dfc12ff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label set: ['B-CONST_DIR', 'B-LIMIT', 'B-OBJ_DIR', 'B-OBJ_NAME', 'B-PARAM', 'B-VAR', 'I-CONST_DIR', 'I-LIMIT', 'I-OBJ_NAME', 'I-PARAM', 'I-VAR', 'O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3d7053c900432d97200f6f5f464f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/714 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8689dcd3bec4253b0a0855484cd9d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4feb1d057b2344bf9a2bbee9fd71e195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\lonel\\AppData\\Local\\Temp\\ipykernel_43400\\2866782259.py:164: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 01:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 0.2724146246910095, 'eval_precision': 0.8777260018639329, 'eval_recall': 0.8166840097121054, 'eval_f1': 0.8461054712065402, 'eval_accuracy': 0.938971528362491, 'eval_runtime': 1.6785, 'eval_samples_per_second': 172.778, 'eval_steps_per_second': 11.32, 'epoch': 3.0}\n",
      "Tokens: ['-DOCSTART-']\n",
      "Predicted Labels: ['O']\n",
      "\n",
      "Tokens: ['A', 'flooring', 'company', 'produces', 'engineered', 'hardwood', 'and', 'vinyl', 'planks', '.', 'Their', 'sales', 'forecasts', 'show', 'an', 'expected', 'demand', 'of', 'at', 'least', '20,000', 'square', 'foot', 'of', 'hardwood', 'and', '10,000', 'square', 'feet', 'of', 'vinyl', 'planks', 'each', 'week', '.', 'To', 'satisfy', 'a', 'shipping', 'contract', ',', 'a', 'total', 'of', 'at', 'least', '60,000', 'square', 'feet', 'of', 'flooring', 'much', 'be', 'shipped', 'each', 'week', '.', 'Due', 'to', 'a', 'labor', 'shortage', 'issue', ',', 'no', 'more', 'than', '50,000', 'square', 'feet', 'of', 'hardwood', 'and', '30,000', 'square', 'feet', 'of', 'vinyl', 'can', 'be', 'produced', 'weekly', '.', 'If', 'a', 'square', 'foot', 'of', 'hardwood', 'flooring', 'yields', 'a', 'profit', 'of', '$', '2.5', 'and', 'a', 'square', 'foot', 'of', 'vinyl', 'planks', 'produces', 'a', '$', '3', 'profit', ',', 'how', 'many', 'of', 'each', 'type', 'of', 'flooring', 'should', 'be', 'made', 'weekly', 'to', 'maximize', 'the', 'company', \"'s\", 'profit', '?']\n",
      "Predicted Labels: ['O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'I-CONST_DIR', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'O', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'O', 'B-CONST_DIR', 'I-CONST_DIR', 'B-LIMIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'I-CONST_DIR', 'I-CONST_DIR', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'O', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'B-OBJ_NAME', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'O', 'B-PARAM', 'B-OBJ_NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OBJ_DIR', 'O', 'O', 'O', 'B-OBJ_NAME', 'O']\n",
      "\n",
      "Tokens: ['John', 'has', 'a', '300', 'acre', 'berry', 'farm', 'on', 'which', 'to', 'plant', 'blueberries', 'and', 'raspberries', '.', 'John', 'has', '$', '10000', 'to', 'spend', 'on', 'watering', 'and', '575', 'days', 'worth', 'of', 'labor', 'available', '.', 'For', 'each', 'acre', 'of', 'blueberries', ',', '6', 'days', 'worth', 'of', 'labor', 'and', '$', '22', 'in', 'watering', 'costs', 'is', 'required', '.', 'For', 'each', 'acre', 'of', 'raspberries', ',', '3', 'days', 'worth', 'of', 'labor', 'and', '$', '25', 'in', 'watering', 'costs', 'is', 'required', '.', 'The', 'profit', 'per', 'acre', 'of', 'blueberries', 'is', '$', '56', 'and', 'the', 'profit', 'per', 'acre', 'of', 'raspberries', 'is', '$', '75', '.', 'Formulate', 'an', 'LP', 'problem', 'in', 'order', 'to', 'maximize', 'profit', '.']\n",
      "Predicted Labels: ['O', 'O', 'O', 'B-LIMIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-VAR', 'O', 'O', 'O', 'O', 'B-LIMIT', 'O', 'O', 'O', 'O', 'O', 'B-LIMIT', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OBJ_NAME', 'O', 'O', 'O', 'B-VAR', 'O', 'O', 'B-PARAM', 'O', 'O', 'B-OBJ_NAME', 'O', 'O', 'O', 'B-VAR', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OBJ_DIR', 'B-OBJ_NAME', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForTokenClassification,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          TrainingArguments,\n",
    "                          Trainer)\n",
    "import evaluate\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Helper function to read CoNLL format data from a file.\n",
    "# Each sentence is separated by an empty line.\n",
    "# Assumes token is in the first column and the entity tag in the last column.\n",
    "# ----------------------------------------------------------\n",
    "def read_conll(filename):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # end of sentence\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    tags.append(labels)\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                # split line; token is first, tag is last column\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[-1])\n",
    "        if tokens:  # if last sentence is not followed by a newline\n",
    "            sentences.append(tokens)\n",
    "            tags.append(labels)\n",
    "    return sentences, tags\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load the train, dev, and test data from files.\n",
    "# ----------------------------------------------------------\n",
    "train_tokens, train_tags = read_conll(r\"D:\\LLM\\DATA\\train.txt\")\n",
    "dev_tokens, dev_tags = read_conll(r\"D:\\LLM\\DATA\\dev.txt\")\n",
    "test_tokens, test_tags = read_conll(r\"D:\\LLM\\DATA\\test.txt\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_tokens, \"labels\": train_tags})\n",
    "dev_dataset   = Dataset.from_dict({\"tokens\": dev_tokens, \"labels\": dev_tags})\n",
    "test_dataset  = Dataset.from_dict({\"tokens\": test_tokens, \"labels\": test_tags})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": dev_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Build the label mapping.\n",
    "# We extract the set of all unique labels from the training set.\n",
    "# ----------------------------------------------------------\n",
    "unique_labels = set()\n",
    "for seq in train_tags:\n",
    "    unique_labels.update(seq)\n",
    "label_list = sorted(list(unique_labels))\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Label set:\", label_list)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load the XLM-RoBERTa tokenizer.\n",
    "# ----------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Tokenize the data and align the labels.\n",
    "# For sub-word tokens, we assign a label only to the first sub-token and -100 to the remaining (ignored in loss).\n",
    "# ----------------------------------------------------------\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(batch[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[labels[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization to the entire dataset.\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load the XLM-RoBERTa-base model for token classification.\n",
    "# Set the number of output labels and provide label mappings.\n",
    "# ----------------------------------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\",\n",
    "                                                        num_labels=len(label_list),\n",
    "                                                        id2label=id_to_label,\n",
    "                                                        label2id=label_to_id)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define training arguments.\n",
    "# ----------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm_roberta_token_classification\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\"  # or remove if your version does not support it either\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define the data collator for token classification.\n",
    "# It dynamically pads the input sequences.\n",
    "# ----------------------------------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define the evaluation metric using the seqeval library.\n",
    "# ----------------------------------------------------------\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for pred, label in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for pred, label in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Initialize the Trainer.\n",
    "# ----------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Train the model.\n",
    "# ----------------------------------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate the model on the test set.\n",
    "# ----------------------------------------------------------\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# (Optional) Predict on the test set.\n",
    "# ----------------------------------------------------------\n",
    "predictions, _, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# (Optional) Post-process and print a few example predictions.\n",
    "for i in range(3):\n",
    "    tokens = tokenized_datasets[\"test\"][i][\"tokens\"]\n",
    "    pred_label_ids = predictions[i]\n",
    "    # Convert sub-token predictions back to word-level labels.\n",
    "    word_ids = tokenized_datasets[\"test\"][i].get(\"word_ids\", None)\n",
    "    if word_ids is None:\n",
    "        # If word_ids are not stored, re-run tokenizer for the single example.\n",
    "        encoded = tokenizer(tokens, is_split_into_words=True)\n",
    "        word_ids = encoded.word_ids()\n",
    "    word_preds = []\n",
    "    previous = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        if word_idx != previous:\n",
    "            word_preds.append(id_to_label[pred_label_ids[idx]])\n",
    "            previous = word_idx\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Predicted Labels:\", word_preds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac89f6a-1eea-4e45-a576-9aa6f211e0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./xlmr_lp_model_1\\\\tokenizer_config.json',\n",
       " './xlmr_lp_model_1\\\\special_tokens_map.json',\n",
       " './xlmr_lp_model_1\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer to the specified directory\n",
    "model.save_pretrained(\"./xlmr_lp_model_1\")\n",
    "tokenizer.save_pretrained(\"./xlmr_lp_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cacd33d-13ad-4158-b060-dfc28a0bf0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load the post-trained XLM-RoBERTa model and tokenizer for sub-task 1\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./xlmr_lp_model_1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./xlmr_lp_model_1\")\n",
    "\n",
    "def predict_entities(text: str, max_length=512):\n",
    "    \"\"\"\n",
    "    Given a plain-text optimization problem description, this function tokenizes the text,\n",
    "    runs the model to predict BIO tags, and returns two lists:\n",
    "      - words: the original tokens (word-level)\n",
    "      - predicted_tags: the predicted label for each word (taking the first sub-token only)\n",
    "    \"\"\"\n",
    "    # Simple whitespace tokenization: for sub-task 1 the input is a plain text description.\n",
    "    words = text.split()\n",
    "    \n",
    "    # Tokenize the list of words while preserving word boundaries.\n",
    "    encoded = tokenizer(words,\n",
    "                        is_split_into_words=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=max_length)\n",
    "    encoded = encoded.to(model.device)\n",
    "    \n",
    "    # Obtain logits from the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded).logits  # shape: (1, seq_length, num_labels)\n",
    "    \n",
    "    # Choose the label with the highest logit for each token (sub-token)\n",
    "    predictions = np.argmax(logits.cpu().detach().numpy(), axis=2)[0]\n",
    "    \n",
    "    # Get the mapping of sub-tokens to original word indices.\n",
    "    word_ids = encoded.word_ids(batch_index=0)\n",
    "    \n",
    "    final_words = []\n",
    "    final_tags = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        # Only take the first sub-token for each word (to avoid duplicate labels for a single word)\n",
    "        if word_idx != previous_word_idx:\n",
    "            final_words.append(words[word_idx])\n",
    "            final_tags.append(model.config.id2label[predictions[idx]])\n",
    "            previous_word_idx = word_idx\n",
    "    return final_words, final_tags\n",
    "\n",
    "def get_conll_format(words, tags):\n",
    "    \"\"\"\n",
    "    Generate a string in CoNLL format with each token on a new line.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    # DOCSTART header.\n",
    "    lines.append(\"-DOCSTART-\\t_\\t_\\tO\\n\")\n",
    "    for word, tag in zip(words, tags):\n",
    "        lines.append(f\"{word}\\t_\\t_\\t{tag}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def save_conll_format(output_str, filename):\n",
    "    \"\"\"\n",
    "    Save the given CoNLL-style string to the specified file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9339bfab-08d7-4e15-b135-4f5129d2af0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output in CoNLL Format:\n",
      "\n",
      "-DOCSTART-\t_\t_\tO\n",
      "\n",
      "A\t_\t_\tO\n",
      "man\t_\t_\tO\n",
      "only\t_\t_\tO\n",
      "eats\t_\t_\tO\n",
      "vegetable\t_\t_\tB-VAR\n",
      "and\t_\t_\tO\n",
      "fruits.\t_\t_\tB-VAR\n",
      "A\t_\t_\tO\n",
      "serving\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "vegetables\t_\t_\tB-VAR\n",
      "contains\t_\t_\tO\n",
      "2\t_\t_\tB-PARAM\n",
      "units\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "vitamins\t_\t_\tO\n",
      "and\t_\t_\tO\n",
      "3\t_\t_\tB-PARAM\n",
      "units\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "minerals.\t_\t_\tO\n",
      "A\t_\t_\tO\n",
      "serving\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "fruit\t_\t_\tB-VAR\n",
      "contains\t_\t_\tO\n",
      "4\t_\t_\tB-PARAM\n",
      "units\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "vitamins\t_\t_\tO\n",
      "and\t_\t_\tO\n",
      "1\t_\t_\tB-PARAM\n",
      "unit\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "minerals.\t_\t_\tO\n",
      "He\t_\t_\tO\n",
      "wants\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "eat\t_\t_\tO\n",
      "at\t_\t_\tB-CONST_DIR\n",
      "least\t_\t_\tI-CONST_DIR\n",
      "20\t_\t_\tB-LIMIT\n",
      "units\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "vitamins\t_\t_\tO\n",
      "and\t_\t_\tO\n",
      "30\t_\t_\tB-LIMIT\n",
      "units\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "minerals.\t_\t_\tO\n",
      "If\t_\t_\tO\n",
      "vegetables\t_\t_\tB-VAR\n",
      "cost\t_\t_\tB-OBJ_NAME\n",
      "$3\t_\t_\tB-PARAM\n",
      "per\t_\t_\tO\n",
      "serving\t_\t_\tO\n",
      "and\t_\t_\tO\n",
      "fruits\t_\t_\tB-VAR\n",
      "cost\t_\t_\tB-OBJ_NAME\n",
      "$5\t_\t_\tB-PARAM\n",
      "per\t_\t_\tO\n",
      "serving,\t_\t_\tO\n",
      "how\t_\t_\tO\n",
      "many\t_\t_\tO\n",
      "servings\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "each\t_\t_\tO\n",
      "should\t_\t_\tO\n",
      "he\t_\t_\tO\n",
      "eat\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "minimize\t_\t_\tB-OBJ_DIR\n",
      "his\t_\t_\tO\n",
      "cost?\t_\t_\tB-OBJ_NAME\n",
      "\n",
      "Saved the predicted output to single_test_output-sub1.conll\n"
     ]
    }
   ],
   "source": [
    "# --- Sample Input for Sub-task 1 ---\n",
    "# For sub-task 1 the input is simply the problem description (without additional XML markup).\n",
    "sample_input = (\n",
    "    \"A man  only eats vegetable and fruits. A serving of vegetables contains 2 units of vitamins and 3 units of minerals. A serving of fruit contains 4 units of vitamins and 1 unit of minerals. He wants to eat at least 20 units of vitamins and 30 units of minerals. If vegetables cost $3 per serving and fruits cost $5 per serving, how many servings of each should he eat to minimize his cost?\"\n",
    ")\n",
    "\n",
    "# Get predictions from the model.\n",
    "words, predicted_tags = predict_entities(sample_input)\n",
    "\n",
    "# Convert the predictions to CoNLL format.\n",
    "conll_output = get_conll_format(words, predicted_tags)\n",
    "print(\"Predicted Output in CoNLL Format:\\n\")\n",
    "print(conll_output)\n",
    "\n",
    "# Save the output to a file.\n",
    "output_filename = \"single_test_output-sub1.conll\"\n",
    "save_conll_format(conll_output, output_filename)\n",
    "print(f\"\\nSaved the predicted output to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "262899b2-ada0-486f-ac91-3457160e48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python conll2bart_ready.py \\\n",
    "  --conll D:/LLM/NER/nl4opt-subtask1-baseline/single_test_output-sub1.conll \\\n",
    "  --out bart_inputs_single_test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99311f1-fde9-4052-9eed-cdcb6f993d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
