{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac39b466-834a-4281-9c5f-2fe9874b4b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted data written to D:\\LLM\\NER\\nl4opt-subtask1-baseline\\train_subtask2_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_conll(filename):\n",
    "    \"\"\"\n",
    "    Reads a file in CoNLL format.\n",
    "    Returns a list of examples where each example is a tuple:\n",
    "      (tokens, labels)\n",
    "    Tokens: list of tokens (strings)\n",
    "    Labels: list of corresponding BIO labels\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # blank line indicates end of a sentence/example\n",
    "                if tokens:\n",
    "                    examples.append((tokens, labels))\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[-1])\n",
    "        if tokens:  # add last sentence if file does not end with a newline\n",
    "            examples.append((tokens, labels))\n",
    "    return examples\n",
    "\n",
    "def extract_entities(tokens, labels):\n",
    "    \"\"\"\n",
    "    Converts BIO tags into a list of entities.\n",
    "    Each entity is represented as a dictionary with:\n",
    "      - \"entity_type\": the entity category (without the \"B-\" or \"I-\" prefix)\n",
    "      - \"entity\": the concatenated token span (joined by a space)\n",
    "      - \"start\": the start token index in the sentence (optional)\n",
    "      - \"end\": the end token index (exclusive; optional)\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    entity = None\n",
    "    start_idx = None\n",
    "    for idx, (token, tag) in enumerate(zip(tokens, labels)):\n",
    "        if tag == \"O\":\n",
    "            # If we were in an entity, save it\n",
    "            if entity is not None:\n",
    "                entities.append({\n",
    "                    \"entity_type\": entity,\n",
    "                    \"entity\": \" \".join(current_tokens),\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": idx\n",
    "                })\n",
    "                entity = None\n",
    "                current_tokens = []\n",
    "            continue\n",
    "\n",
    "        # Split tag into prefix and entity type, e.g., \"B-VAR\" -> (\"B\", \"VAR\")\n",
    "        try:\n",
    "            prefix, ent_type = tag.split(\"-\", 1)\n",
    "        except ValueError:\n",
    "            # In case the tag does not follow the conventional format; skip it.\n",
    "            continue\n",
    "\n",
    "        if prefix == \"B\":  # beginning of a new entity span\n",
    "            if entity is not None:  # save the previous entity span\n",
    "                entities.append({\n",
    "                    \"entity_type\": entity,\n",
    "                    \"entity\": \" \".join(current_tokens),\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": idx\n",
    "                })\n",
    "            entity = ent_type\n",
    "            current_tokens = [token]\n",
    "            start_idx = idx\n",
    "        elif prefix == \"I\" and entity == ent_type:\n",
    "            # Continuation of an entity span\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            # Case: tag inconsistency (e.g., I- tag that doesn’t match the previous B- tag).\n",
    "            # We start a new entity span.\n",
    "            if entity is not None:\n",
    "                entities.append({\n",
    "                    \"entity_type\": entity,\n",
    "                    \"entity\": \" \".join(current_tokens),\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": idx\n",
    "                })\n",
    "            entity = ent_type\n",
    "            current_tokens = [token]\n",
    "            start_idx = idx\n",
    "\n",
    "    # Catch any remaining entity at the end of the sentence\n",
    "    if entity is not None:\n",
    "        entities.append({\n",
    "            \"entity_type\": entity,\n",
    "            \"entity\": \" \".join(current_tokens),\n",
    "            \"start\": start_idx,\n",
    "            \"end\": len(tokens)\n",
    "        })\n",
    "    return entities\n",
    "\n",
    "def convert_to_subtask2_format(conll_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Reads CoNLL-formatted data (sub-task 1 training data) and converts it to the format\n",
    "    expected by sub-task 2. For each example, it produces a JSON object with:\n",
    "      - \"problem_description\": the full natural language text (tokens joined together)\n",
    "      - \"entities\": a list of problem entities extracted using the BIO tags.\n",
    "    Saves the results in JSON Lines format.\n",
    "    \"\"\"\n",
    "    examples = read_conll(conll_filename)\n",
    "    with open(output_filename, 'w', encoding='utf-8') as out_file:\n",
    "        for tokens, labels in examples:\n",
    "            problem_description = \" \".join(tokens)\n",
    "            entities = extract_entities(tokens, labels)\n",
    "            # Create a dict matching the expected sub-task 2 input format.\n",
    "            # (If needed, you can also add an \"order_mapping\" field here.)\n",
    "            formatted_example = {\n",
    "                \"problem_description\": problem_description,\n",
    "                \"entities\": entities\n",
    "            }\n",
    "            out_file.write(json.dumps(formatted_example) + \"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "conll_train_file = r\"D:\\LLM\\NER\\nl4opt-subtask1-baseline\\test_output.conll\"      # sub-task 1 data (CoNLL format)\n",
    "output_jsonl_file = r\"D:\\LLM\\NER\\nl4opt-subtask1-baseline\\train_subtask2_test.jsonl\"  # desired output format for sub-task 2\n",
    "\n",
    "convert_to_subtask2_format(conll_train_file, output_jsonl_file)\n",
    "print(f\"Converted data written to {output_jsonl_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fc35773-56bc-456a-9bad-aeebb6308582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Output saved to D:\\LLM\\NER\\nl4opt-subtask1-baseline\\train_subtask2_test_2.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_conll(filename):\n",
    "    \"\"\"\n",
    "    Reads a CoNLL-formatted file.\n",
    "    Returns a list of examples; each example is a tuple (tokens, labels).\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # empty line signals end of one example\n",
    "                if tokens:\n",
    "                    examples.append((tokens, labels))\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                # Assuming the token is the first column and the tag is the last.\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[-1])\n",
    "        if tokens:\n",
    "            examples.append((tokens, labels))\n",
    "    return examples\n",
    "\n",
    "def extract_entities(tokens, labels):\n",
    "    \"\"\"\n",
    "    Extracts entity spans from a token list with BIO tags.\n",
    "    Each entity is returned as a dictionary containing:\n",
    "       - \"text\": the concatenated tokens (joined with a space)\n",
    "       - \"token_start\": the index of the first token of the entity\n",
    "       - \"token_end\": the index (exclusive) of the entity\n",
    "       - \"start\": the character start (here, we use token_start as a proxy)\n",
    "       - \"end\": the character end (here, we use token_end as a proxy)\n",
    "       - \"label\": the entity type (without the B- or I- prefix)\n",
    "    \"\"\"\n",
    "    spans = []\n",
    "    current_entity = None\n",
    "    current_tokens = []\n",
    "    start_idx = None\n",
    "\n",
    "    for idx, (token, tag) in enumerate(zip(tokens, labels)):\n",
    "        if tag == \"O\":\n",
    "            if current_entity is not None:\n",
    "                spans.append({\n",
    "                    \"text\": \" \".join(current_tokens),\n",
    "                    \"token_start\": start_idx,\n",
    "                    \"token_end\": idx,\n",
    "                    \"start\": start_idx,  # simplified proxy for character offset\n",
    "                    \"end\": idx,\n",
    "                    \"label\": current_entity\n",
    "                })\n",
    "                current_entity = None\n",
    "                current_tokens = []\n",
    "                start_idx = None\n",
    "            continue\n",
    "\n",
    "        # Split tag: e.g., \"B-VAR\" -> prefix \"B\", entity \"VAR\"\n",
    "        try:\n",
    "            prefix, ent_type = tag.split(\"-\", 1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_entity is not None:\n",
    "                # Save previous entity span\n",
    "                spans.append({\n",
    "                    \"text\": \" \".join(current_tokens),\n",
    "                    \"token_start\": start_idx,\n",
    "                    \"token_end\": idx,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": idx,\n",
    "                    \"label\": current_entity\n",
    "                })\n",
    "            current_entity = ent_type\n",
    "            current_tokens = [token]\n",
    "            start_idx = idx\n",
    "        elif prefix == \"I\" and current_entity == ent_type:\n",
    "            current_tokens.append(token)\n",
    "        else:\n",
    "            # If the tag doesn't follow the expected sequence, end the previous span and start a new one.\n",
    "            if current_entity is not None:\n",
    "                spans.append({\n",
    "                    \"text\": \" \".join(current_tokens),\n",
    "                    \"token_start\": start_idx,\n",
    "                    \"token_end\": idx,\n",
    "                    \"start\": start_idx,\n",
    "                    \"end\": idx,\n",
    "                    \"label\": current_entity\n",
    "                })\n",
    "            current_entity = ent_type\n",
    "            current_tokens = [token]\n",
    "            start_idx = idx\n",
    "\n",
    "    # End of sentence: add remaining entity if any.\n",
    "    if current_entity is not None:\n",
    "        spans.append({\n",
    "            \"text\": \" \".join(current_tokens),\n",
    "            \"token_start\": start_idx,\n",
    "            \"token_end\": len(tokens),\n",
    "            \"start\": start_idx,\n",
    "            \"end\": len(tokens),\n",
    "            \"label\": current_entity\n",
    "        })\n",
    "    return spans\n",
    "\n",
    "def build_order_mapping(vars_list):\n",
    "    \"\"\"\n",
    "    Creates a mapping from a canonical variable name (from vars_list) to its order index.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for idx, var in enumerate(vars_list):\n",
    "        mapping[var] = idx\n",
    "    return mapping\n",
    "\n",
    "def convert_subtask1_to_subtask2(conll_filename, output_filename):\n",
    "    \"\"\"\n",
    "    Converts sub-task 1 output (in CoNLL format) to sub-task 2 input format (JSON format).\n",
    "    The output JSON object contains:\n",
    "      - \"document\": the original problem description (tokens joined into one string)\n",
    "      - \"tokens\": the token list\n",
    "      - \"spans\": the extracted entity spans (each a dict with text, token_start, token_end, etc.)\n",
    "      - \"vars\": a list of unique variable names (from spans labeled \"VAR\")\n",
    "      - \"var_mentions\": a list (in order) of all variable mentions (all spans with label \"VAR\")\n",
    "      - \"params\": a list of all parameters (from spans labeled \"PARAM\")\n",
    "      - \"var_mention_to_first_var\": mapping from each variable mention to the first occurrence\n",
    "      - \"first_var_to_mentions\": reverse mapping from canonical variable to list of mentions\n",
    "      - \"obj_declaration\": a stub example built from available objective spans (labels \"OBJ_DIR\" and \"OBJ_NAME\") and parameters\n",
    "      - \"const_declarations\": a stub list built from constraint spans (\"CONST_DIR\" and \"LIMIT\")\n",
    "      - \"order_mapping\": mapping of each variable (canonical) to an order index\n",
    "    The top-level JSON object is keyed by a unique id (here we use hash(document)).\n",
    "    \"\"\"\n",
    "    examples = read_conll(conll_filename)\n",
    "    output_dict = {}\n",
    "    \n",
    "    for tokens, labels in examples:\n",
    "        document = \" \".join(tokens)\n",
    "        spans = extract_entities(tokens, labels)\n",
    "        \n",
    "        # Collect variables and parameters from spans.\n",
    "        vars_list = []\n",
    "        var_mentions = []\n",
    "        params = []\n",
    "        for span in spans:\n",
    "            if span[\"label\"] == \"VAR\":\n",
    "                var_mentions.append(span[\"text\"])\n",
    "                if span[\"text\"] not in vars_list:\n",
    "                    vars_list.append(span[\"text\"])\n",
    "            elif span[\"label\"] == \"PARAM\":\n",
    "                params.append(span[\"text\"])\n",
    "                \n",
    "        # Create mapping: for each var mention, map to its first occurrence.\n",
    "        var_mention_to_first_var = {}\n",
    "        first_var_to_mentions = {}\n",
    "        for mention in var_mentions:\n",
    "            if mention not in var_mention_to_first_var:\n",
    "                var_mention_to_first_var[mention] = mention\n",
    "                first_var_to_mentions[mention] = [mention]\n",
    "            else:\n",
    "                first_var_to_mentions[mention].append(mention)\n",
    "                \n",
    "        # Stub: Build objective declaration from spans with label OBJ_DIR and OBJ_NAME.\n",
    "        obj_dir = None\n",
    "        obj_name = None\n",
    "        for span in spans:\n",
    "            if span[\"label\"] == \"OBJ_DIR\":\n",
    "                obj_dir = span[\"text\"]\n",
    "            elif span[\"label\"] == \"OBJ_NAME\":\n",
    "                if not obj_name:\n",
    "                    obj_name = span[\"text\"]\n",
    "                else:\n",
    "                    obj_name += \" \" + span[\"text\"]\n",
    "        # For terms, we map each variable to a parameter if available.\n",
    "        terms = {}\n",
    "        if params and vars_list:\n",
    "            # This is a simple heuristic: assign the first PARAM to the first variable, etc.\n",
    "            for i, var in enumerate(vars_list):\n",
    "                if i < len(params):\n",
    "                    terms[var] = params[i]\n",
    "        obj_declaration = {\n",
    "            \"type\": \"objective\",\n",
    "            \"direction\": obj_dir if obj_dir else \"\",\n",
    "            \"name\": obj_name if obj_name else \"\",\n",
    "            \"terms\": terms\n",
    "        }\n",
    "        \n",
    "        # Stub: Build a list of constraint declarations.\n",
    "        const_declarations = []\n",
    "        # For every span with label CONST_DIR, try to pair with a nearby LIMIT span.\n",
    "        for span in spans:\n",
    "            if span[\"label\"] == \"CONST_DIR\":\n",
    "                # Look ahead for a LIMIT span after this.\n",
    "                for span2 in spans:\n",
    "                    if span2[\"label\"] == \"LIMIT\" and span2[\"token_start\"] > span[\"token_start\"]:\n",
    "                        # Simple heuristic: if we have a CONST_DIR followed by a LIMIT, create a constraint.\n",
    "                        # Decide the type of constraint (e.g., \"sum\" or \"ratio\") based on the text of CONST_DIR.\n",
    "                        const_declarations.append({\n",
    "                            \"type\": \"ratio\" if \"minimum\" in span[\"text\"].lower() or \"no more than\" in span[\"text\"].lower() else \"sum\",\n",
    "                            \"direction\": span[\"text\"],\n",
    "                            \"limit\": span2[\"text\"],\n",
    "                            # In a more complete solution, you would also assign a variable for ratio constraints.\n",
    "                            \"operator\": \"GREATER_OR_EQUAL\" if \"minimum\" in span[\"text\"].lower() else \"LESS_OR_EQUAL\"\n",
    "                        })\n",
    "                        break\n",
    "        \n",
    "        # Determine the order mapping for variables.\n",
    "        order_mapping = build_order_mapping(vars_list)\n",
    "        \n",
    "        # Build the JSON object for this example.\n",
    "        example_json = {\n",
    "            \"document\": document,\n",
    "            \"tokens\": tokens,\n",
    "            \"spans\": spans,\n",
    "            \"vars\": vars_list,\n",
    "            \"var_mentions\": var_mentions,\n",
    "            \"params\": params,\n",
    "            \"var_mention_to_first_var\": var_mention_to_first_var,\n",
    "            \"first_var_to_mentions\": first_var_to_mentions,\n",
    "            \"obj_declaration\": obj_declaration,\n",
    "            \"const_declarations\": const_declarations,\n",
    "            \"order_mapping\": order_mapping\n",
    "        }\n",
    "        \n",
    "        # Use a unique id key for this document (for example, using the hash of the document).\n",
    "        doc_id = str(hash(document))\n",
    "        output_dict[doc_id] = example_json\n",
    "\n",
    "    # Save the output as a single JSON object (or use JSON Lines as needed)\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(output_dict, out_file, indent=2)\n",
    "\n",
    "# Example usage:\n",
    "conll_train_file = r\"D:\\LLM\\NER\\nl4opt-subtask1-baseline\\test_output.conll\"          # Sub-task 1 data in CoNLL format\n",
    "output_jsonl_file = r\"D:\\LLM\\NER\\nl4opt-subtask1-baseline\\train_subtask2_test_2.jsonl\"  # Output file for sub-task 2 input\n",
    "\n",
    "convert_subtask1_to_subtask2(conll_train_file, output_jsonl_file)\n",
    "print(f\"Conversion complete. Output saved to {output_jsonl_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37749f72-5aec-48b0-a1a4-86232ff3aacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-DOCSTART-\t_\t_\tO\n",
      "\n",
      "Cautious\t_\t_\tO\n",
      "Asset\t_\t_\tO\n",
      "Investment\t_\t_\tO\n",
      "has\t_\t_\tO\n",
      "a\t_\t_\tO\n",
      "total\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "$150,000\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "manage\t_\t_\tO\n",
      "and\t_\t_\tO\n",
      "decides\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "invest\t_\t_\tO\n",
      "it\t_\t_\tO\n",
      "in\t_\t_\tO\n",
      "money\t_\t_\tB-VAR\n",
      "market\t_\t_\tI-VAR\n",
      "fund,\t_\t_\tI-VAR\n",
      "which\t_\t_\tO\n",
      "yields\t_\t_\tO\n",
      "a\t_\t_\tO\n",
      "2%\t_\t_\tB-PARAM\n",
      "return\t_\t_\tB-OBJ_NAME\n",
      "as\t_\t_\tO\n",
      "well\t_\t_\tO\n",
      "as\t_\t_\tO\n",
      "in\t_\t_\tO\n",
      "foreign\t_\t_\tB-VAR\n",
      "bonds,\t_\t_\tI-VAR\n",
      "which\t_\t_\tO\n",
      "gives\t_\t_\tO\n",
      "and\t_\t_\tO\n",
      "average\t_\t_\tO\n",
      "rate\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "return\t_\t_\tB-OBJ_NAME\n",
      "of\t_\t_\tO\n",
      "10.2%.\t_\t_\tB-PARAM\n",
      "Internal\t_\t_\tO\n",
      "policies\t_\t_\tO\n",
      "require\t_\t_\tO\n",
      "PAI\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "diversify\t_\t_\tO\n",
      "the\t_\t_\tO\n",
      "asset\t_\t_\tO\n",
      "allocation\t_\t_\tO\n",
      "so\t_\t_\tO\n",
      "that\t_\t_\tO\n",
      "the\t_\t_\tO\n",
      "minimum\t_\t_\tB-CONST_DIR\n",
      "investment\t_\t_\tO\n",
      "in\t_\t_\tO\n",
      "money\t_\t_\tB-VAR\n",
      "market\t_\t_\tI-VAR\n",
      "fund\t_\t_\tI-VAR\n",
      "is\t_\t_\tO\n",
      "40%\t_\t_\tB-LIMIT\n",
      "of\t_\t_\tO\n",
      "the\t_\t_\tO\n",
      "total\t_\t_\tO\n",
      "investment.\t_\t_\tO\n",
      "Due\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "the\t_\t_\tO\n",
      "risk\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "default\t_\t_\tO\n",
      "of\t_\t_\tO\n",
      "foreign\t_\t_\tO\n",
      "countries,\t_\t_\tO\n",
      "no\t_\t_\tB-CONST_DIR\n",
      "more\t_\t_\tI-CONST_DIR\n",
      "than\t_\t_\tI-CONST_DIR\n",
      "40%\t_\t_\tB-LIMIT\n",
      "of\t_\t_\tO\n",
      "the\t_\t_\tO\n",
      "total\t_\t_\tO\n",
      "investment\t_\t_\tO\n",
      "should\t_\t_\tO\n",
      "be\t_\t_\tO\n",
      "allocated\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "foreign\t_\t_\tB-VAR\n",
      "bonds.\t_\t_\tI-VAR\n",
      "How\t_\t_\tO\n",
      "much\t_\t_\tO\n",
      "should\t_\t_\tO\n",
      "the\t_\t_\tO\n",
      "Cautious\t_\t_\tO\n",
      "Asset\t_\t_\tO\n",
      "Investment\t_\t_\tO\n",
      "allocate\t_\t_\tO\n",
      "in\t_\t_\tO\n",
      "each\t_\t_\tO\n",
      "asset\t_\t_\tO\n",
      "so\t_\t_\tO\n",
      "as\t_\t_\tO\n",
      "to\t_\t_\tO\n",
      "maximize\t_\t_\tB-OBJ_DIR\n",
      "its\t_\t_\tO\n",
      "average\t_\t_\tO\n",
      "return?\t_\t_\tB-OBJ_NAME\n"
     ]
    }
   ],
   "source": [
    "print(conll_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1134452-6b31-4b58-af88-55a2579e7a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 714 examples from D:\\LLM\\DATA\\train.txt\n",
      "Saved 714 examples to D:\\LLM\\DATA\\train_bart_ready_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# --- Helper: Read CoNLL data from file ---\n",
    "def read_conll(filename):\n",
    "    \"\"\"Read a CoNLL-format file and return a list of sentences and corresponding tag sequences.\"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    tags.append(labels)\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[-1])\n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            tags.append(labels)\n",
    "    return sentences, tags\n",
    "\n",
    "\n",
    "# --- Helper: Convert a BIO-tagged sentence to an XML-like annotated string ---\n",
    "def bio_to_xml(tokens, bio_tags):\n",
    "    \"\"\"\n",
    "    Convert tokens and their BIO tags to an XML-like annotated string.\n",
    "    For example, if tokens = ['A', 'foldable', 'cell-phone'] with tags\n",
    "      ['O', 'B-VAR', 'I-VAR'],\n",
    "    then the output might be \"A <VAR>foldable cell-phone</VAR>\".\n",
    "    \"\"\"\n",
    "    output_tokens = []\n",
    "    current_entity = None  # e.g., \"VAR\", \"CONSTR_DIR\", etc.\n",
    "\n",
    "    for token, tag in zip(tokens, bio_tags):\n",
    "        if tag == \"O\":\n",
    "            # close any open entity span\n",
    "            if current_entity is not None:\n",
    "                output_tokens[-1] += f\"</{current_entity}>\"\n",
    "                current_entity = None\n",
    "            output_tokens.append(token)\n",
    "        else:\n",
    "            # tag in BIO format; split into prefix and entity type\n",
    "            try:\n",
    "                prefix, entity_type = tag.split(\"-\")\n",
    "            except ValueError:\n",
    "                # If tag is malformed, treat as O.\n",
    "                if current_entity is not None:\n",
    "                    output_tokens[-1] += f\"</{current_entity}>\"\n",
    "                    current_entity = None\n",
    "                output_tokens.append(token)\n",
    "                continue\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                # If an entity span is already open, close it first.\n",
    "                if current_entity is not None:\n",
    "                    output_tokens[-1] += f\"</{current_entity}>\"\n",
    "                # Open new entity span with the detected entity type.\n",
    "                output_tokens.append(f\"<{entity_type}>{token}\")\n",
    "                current_entity = entity_type\n",
    "            elif prefix == \"I\":\n",
    "                # Continue the entity span\n",
    "                if current_entity == entity_type:\n",
    "                    output_tokens.append(token)\n",
    "                else:\n",
    "                    # Inconsistent tag ordering; treat as beginning of new entity.\n",
    "                    if current_entity is not None:\n",
    "                        output_tokens[-1] += f\"</{current_entity}>\"\n",
    "                    output_tokens.append(f\"<{entity_type}>{token}\")\n",
    "                    current_entity = entity_type\n",
    "    # Close any open entity span at the end.\n",
    "    if current_entity is not None:\n",
    "        output_tokens[-1] += f\"</{current_entity}>\"\n",
    "    # Join tokens with a single space (or use your desired formatting).\n",
    "    return \" \".join(output_tokens)\n",
    "\n",
    "\n",
    "# --- Main script: Process train.txt and produce train_bart_ready_1.jsonl ---\n",
    "def main():\n",
    "    # Path to input file (CoNLL format) and output jsonl file.\n",
    "    input_file = r\"D:\\LLM\\DATA\\train.txt\"\n",
    "    output_file = r\"D:\\LLM\\DATA\\train_bart_ready_1.jsonl\"\n",
    "\n",
    "    # Read tokenized examples and their BIO tags.\n",
    "    sentences, tag_sequences = read_conll(input_file)\n",
    "    print(f\"Read {len(sentences)} examples from {input_file}\")\n",
    "\n",
    "    # Prepare a list of JSON objects\n",
    "    json_objects = []\n",
    "    for tokens, tags in zip(sentences, tag_sequences):\n",
    "        # Reconstruct the original (untokenized) text\n",
    "        # Here we assume that simply joining tokens with a space approximates the original text.\n",
    "        original_text = \" \".join(tokens)\n",
    "        # Convert the tokens/BIO tags to an XML-like annotation.\n",
    "        xml_annotation = bio_to_xml(tokens, tags)\n",
    "        # Prepare the JSON object – here we assume the input for training BART is the XML-annotated text.\n",
    "        # You could also include fields for the target logical form if needed.\n",
    "        json_obj = {\n",
    "            \"input\": xml_annotation,\n",
    "            \"original_text\": original_text  # optionally include the original text for reference\n",
    "        }\n",
    "        json_objects.append(json_obj)\n",
    "\n",
    "    # Write out to a JSONL file.\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        for obj in json_objects:\n",
    "            out_f.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "    print(f\"Saved {len(json_objects)} examples to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb393071-c5e1-49d3-b6a2-db4dfc12ff0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label set: ['B-CONST_DIR', 'B-LIMIT', 'B-OBJ_DIR', 'B-OBJ_NAME', 'B-PARAM', 'B-VAR', 'I-CONST_DIR', 'I-LIMIT', 'I-OBJ_NAME', 'I-PARAM', 'I-VAR', 'O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3d7053c900432d97200f6f5f464f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/714 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8689dcd3bec4253b0a0855484cd9d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4feb1d057b2344bf9a2bbee9fd71e195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "D:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\torch\\cuda\\__init__.py:235: UserWarning: \n",
      "NVIDIA GeForce RTX 5070 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5070 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\lonel\\AppData\\Local\\Temp\\ipykernel_43400\\2866782259.py:164: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='135' max='135' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [135/135 01:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.922800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set evaluation: {'eval_loss': 0.2724146246910095, 'eval_precision': 0.8777260018639329, 'eval_recall': 0.8166840097121054, 'eval_f1': 0.8461054712065402, 'eval_accuracy': 0.938971528362491, 'eval_runtime': 1.6785, 'eval_samples_per_second': 172.778, 'eval_steps_per_second': 11.32, 'epoch': 3.0}\n",
      "Tokens: ['-DOCSTART-']\n",
      "Predicted Labels: ['O']\n",
      "\n",
      "Tokens: ['A', 'flooring', 'company', 'produces', 'engineered', 'hardwood', 'and', 'vinyl', 'planks', '.', 'Their', 'sales', 'forecasts', 'show', 'an', 'expected', 'demand', 'of', 'at', 'least', '20,000', 'square', 'foot', 'of', 'hardwood', 'and', '10,000', 'square', 'feet', 'of', 'vinyl', 'planks', 'each', 'week', '.', 'To', 'satisfy', 'a', 'shipping', 'contract', ',', 'a', 'total', 'of', 'at', 'least', '60,000', 'square', 'feet', 'of', 'flooring', 'much', 'be', 'shipped', 'each', 'week', '.', 'Due', 'to', 'a', 'labor', 'shortage', 'issue', ',', 'no', 'more', 'than', '50,000', 'square', 'feet', 'of', 'hardwood', 'and', '30,000', 'square', 'feet', 'of', 'vinyl', 'can', 'be', 'produced', 'weekly', '.', 'If', 'a', 'square', 'foot', 'of', 'hardwood', 'flooring', 'yields', 'a', 'profit', 'of', '$', '2.5', 'and', 'a', 'square', 'foot', 'of', 'vinyl', 'planks', 'produces', 'a', '$', '3', 'profit', ',', 'how', 'many', 'of', 'each', 'type', 'of', 'flooring', 'should', 'be', 'made', 'weekly', 'to', 'maximize', 'the', 'company', \"'s\", 'profit', '?']\n",
      "Predicted Labels: ['O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'I-CONST_DIR', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'O', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'O', 'B-CONST_DIR', 'I-CONST_DIR', 'B-LIMIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'I-CONST_DIR', 'I-CONST_DIR', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'O', 'B-LIMIT', 'O', 'O', 'O', 'B-VAR', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'B-OBJ_NAME', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'I-VAR', 'O', 'O', 'O', 'B-PARAM', 'B-OBJ_NAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OBJ_DIR', 'O', 'O', 'O', 'B-OBJ_NAME', 'O']\n",
      "\n",
      "Tokens: ['John', 'has', 'a', '300', 'acre', 'berry', 'farm', 'on', 'which', 'to', 'plant', 'blueberries', 'and', 'raspberries', '.', 'John', 'has', '$', '10000', 'to', 'spend', 'on', 'watering', 'and', '575', 'days', 'worth', 'of', 'labor', 'available', '.', 'For', 'each', 'acre', 'of', 'blueberries', ',', '6', 'days', 'worth', 'of', 'labor', 'and', '$', '22', 'in', 'watering', 'costs', 'is', 'required', '.', 'For', 'each', 'acre', 'of', 'raspberries', ',', '3', 'days', 'worth', 'of', 'labor', 'and', '$', '25', 'in', 'watering', 'costs', 'is', 'required', '.', 'The', 'profit', 'per', 'acre', 'of', 'blueberries', 'is', '$', '56', 'and', 'the', 'profit', 'per', 'acre', 'of', 'raspberries', 'is', '$', '75', '.', 'Formulate', 'an', 'LP', 'problem', 'in', 'order', 'to', 'maximize', 'profit', '.']\n",
      "Predicted Labels: ['O', 'O', 'O', 'B-LIMIT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-VAR', 'O', 'O', 'O', 'O', 'B-LIMIT', 'O', 'O', 'O', 'O', 'O', 'B-LIMIT', 'O', 'O', 'O', 'O', 'B-CONST_DIR', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VAR', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OBJ_NAME', 'O', 'O', 'O', 'B-VAR', 'O', 'O', 'B-PARAM', 'O', 'O', 'B-OBJ_NAME', 'O', 'O', 'O', 'B-VAR', 'O', 'O', 'B-PARAM', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-OBJ_DIR', 'B-OBJ_NAME', 'O']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForTokenClassification,\n",
    "                          DataCollatorForTokenClassification,\n",
    "                          TrainingArguments,\n",
    "                          Trainer)\n",
    "import evaluate\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Helper function to read CoNLL format data from a file.\n",
    "# Each sentence is separated by an empty line.\n",
    "# Assumes token is in the first column and the entity tag in the last column.\n",
    "# ----------------------------------------------------------\n",
    "def read_conll(filename):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":  # end of sentence\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    tags.append(labels)\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                # split line; token is first, tag is last column\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[-1])\n",
    "        if tokens:  # if last sentence is not followed by a newline\n",
    "            sentences.append(tokens)\n",
    "            tags.append(labels)\n",
    "    return sentences, tags\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load the train, dev, and test data from files.\n",
    "# ----------------------------------------------------------\n",
    "train_tokens, train_tags = read_conll(r\"D:\\LLM\\DATA\\train.txt\")\n",
    "dev_tokens, dev_tags = read_conll(r\"D:\\LLM\\DATA\\dev.txt\")\n",
    "test_tokens, test_tags = read_conll(r\"D:\\LLM\\DATA\\test.txt\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_tokens, \"labels\": train_tags})\n",
    "dev_dataset   = Dataset.from_dict({\"tokens\": dev_tokens, \"labels\": dev_tags})\n",
    "test_dataset  = Dataset.from_dict({\"tokens\": test_tokens, \"labels\": test_tags})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": dev_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Build the label mapping.\n",
    "# We extract the set of all unique labels from the training set.\n",
    "# ----------------------------------------------------------\n",
    "unique_labels = set()\n",
    "for seq in train_tags:\n",
    "    unique_labels.update(seq)\n",
    "label_list = sorted(list(unique_labels))\n",
    "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "\n",
    "print(\"Label set:\", label_list)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load the XLM-RoBERTa tokenizer.\n",
    "# ----------------------------------------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Tokenize the data and align the labels.\n",
    "# For sub-word tokens, we assign a label only to the first sub-token and -100 to the remaining (ignored in loss).\n",
    "# ----------------------------------------------------------\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized_inputs = tokenizer(batch[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(batch[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_to_id[labels[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the tokenization to the entire dataset.\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Load the XLM-RoBERTa-base model for token classification.\n",
    "# Set the number of output labels and provide label mappings.\n",
    "# ----------------------------------------------------------\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-base\",\n",
    "                                                        num_labels=len(label_list),\n",
    "                                                        id2label=id_to_label,\n",
    "                                                        label2id=label_to_id)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define training arguments.\n",
    "# ----------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlm_roberta_token_classification\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\"  # or remove if your version does not support it either\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define the data collator for token classification.\n",
    "# It dynamically pads the input sequences.\n",
    "# ----------------------------------------------------------\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Define the evaluation metric using the seqeval library.\n",
    "# ----------------------------------------------------------\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for pred, label in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for pred, label in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Initialize the Trainer.\n",
    "# ----------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Train the model.\n",
    "# ----------------------------------------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# Evaluate the model on the test set.\n",
    "# ----------------------------------------------------------\n",
    "test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test set evaluation:\", test_results)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# (Optional) Predict on the test set.\n",
    "# ----------------------------------------------------------\n",
    "predictions, _, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# (Optional) Post-process and print a few example predictions.\n",
    "for i in range(3):\n",
    "    tokens = tokenized_datasets[\"test\"][i][\"tokens\"]\n",
    "    pred_label_ids = predictions[i]\n",
    "    # Convert sub-token predictions back to word-level labels.\n",
    "    word_ids = tokenized_datasets[\"test\"][i].get(\"word_ids\", None)\n",
    "    if word_ids is None:\n",
    "        # If word_ids are not stored, re-run tokenizer for the single example.\n",
    "        encoded = tokenizer(tokens, is_split_into_words=True)\n",
    "        word_ids = encoded.word_ids()\n",
    "    word_preds = []\n",
    "    previous = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        if word_idx != previous:\n",
    "            word_preds.append(id_to_label[pred_label_ids[idx]])\n",
    "            previous = word_idx\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Predicted Labels:\", word_preds)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac89f6a-1eea-4e45-a576-9aa6f211e0f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save the model and tokenizer to the specified directory\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./xlmr_lp_model_1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./xlmr_lp_model_1\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer to the specified directory\n",
    "model.save_pretrained(\"./xlmr_lp_model_1\")\n",
    "tokenizer.save_pretrained(\"./xlmr_lp_model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cacd33d-13ad-4158-b060-dfc28a0bf0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './xlmr_lp_model_1'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './xlmr_lp_model_1'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mHFValidationError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForTokenClassification, AutoTokenizer\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load the post-trained XLM-RoBERTa model and tokenizer for sub-task 1\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./xlmr_lp_model_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./xlmr_lp_model_1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_entities\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, max_length=\u001b[32m512\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:492\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[32m    491\u001b[39m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m         resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    500\u001b[39m         commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\utils\\hub.py:471\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    464\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;66;03m# Now we try to recover if we can find all files correctly in the cache\u001b[39;00m\n\u001b[32m    470\u001b[39m resolved_files = [\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[43m_get_cache_file_to_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m full_filenames\n\u001b[32m    472\u001b[39m ]\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m resolved_files):\n\u001b[32m    474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_files\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\transformers\\utils\\hub.py:134\u001b[39m, in \u001b[36m_get_cache_file_to_return\u001b[39m\u001b[34m(path_or_repo_id, full_filename, cache_dir, revision)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cache_file_to_return\u001b[39m(\n\u001b[32m    131\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m, full_filename: \u001b[38;5;28mstr\u001b[39m, cache_dir: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28;01mNone\u001b[39;00m] = \u001b[38;5;28;01mNone\u001b[39;00m, revision: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m ):\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# We try to see if we have a cached version (not up to date):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     resolved_file = \u001b[43mtry_to_load_from_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resolved_file != _CACHED_NO_EXIST:\n\u001b[32m    136\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:106\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mzip\u001b[39m(signature.parameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[32m    103\u001b[39m     kwargs.items(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[32m    104\u001b[39m ):\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfrom_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mto_id\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m arg_name == \u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    109\u001b[39m         has_token = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\env_opt\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:160\u001b[39m, in \u001b[36mvalidate_repo_id\u001b[39m\u001b[34m(repo_id)\u001b[39m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    155\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must be in the form \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrepo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnamespace/repo_name\u001b[39m\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Use `repo_type` argument if needed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m     )\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX.match(repo_id):\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRepo id must use alphanumeric chars or \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m\u001b[33m are\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m forbidden, \u001b[39m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\u001b[33m cannot start or end the name, max length is 96:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    163\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    164\u001b[39m     )\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot have -- or .. in repo_id: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mHFValidationError\u001b[39m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './xlmr_lp_model_1'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "# Load the post-trained XLM-RoBERTa model and tokenizer for sub-task 1\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"./xlmr_lp_model_1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./xlmr_lp_model_1\")\n",
    "\n",
    "def predict_entities(text: str, max_length=512):\n",
    "    \"\"\"\n",
    "    Given a plain-text optimization problem description, this function tokenizes the text,\n",
    "    runs the model to predict BIO tags, and returns two lists:\n",
    "      - words: the original tokens (word-level)\n",
    "      - predicted_tags: the predicted label for each word (taking the first sub-token only)\n",
    "    \"\"\"\n",
    "    # Simple whitespace tokenization: for sub-task 1 the input is a plain text description.\n",
    "    words = text.split()\n",
    "    \n",
    "    # Tokenize the list of words while preserving word boundaries.\n",
    "    encoded = tokenizer(words,\n",
    "                        is_split_into_words=True,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=max_length)\n",
    "    encoded = encoded.to(model.device)\n",
    "    \n",
    "    # Obtain logits from the model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded).logits  # shape: (1, seq_length, num_labels)\n",
    "    \n",
    "    # Choose the label with the highest logit for each token (sub-token)\n",
    "    predictions = np.argmax(logits.cpu().detach().numpy(), axis=2)[0]\n",
    "    \n",
    "    # Get the mapping of sub-tokens to original word indices.\n",
    "    word_ids = encoded.word_ids(batch_index=0)\n",
    "    \n",
    "    final_words = []\n",
    "    final_tags = []\n",
    "    previous_word_idx = None\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        # Only take the first sub-token for each word (to avoid duplicate labels for a single word)\n",
    "        if word_idx != previous_word_idx:\n",
    "            final_words.append(words[word_idx])\n",
    "            final_tags.append(model.config.id2label[predictions[idx]])\n",
    "            previous_word_idx = word_idx\n",
    "    return final_words, final_tags\n",
    "\n",
    "def get_conll_format(words, tags):\n",
    "    \"\"\"\n",
    "    Generate a string in CoNLL format with each token on a new line.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    # DOCSTART header.\n",
    "    lines.append(\"-DOCSTART-\\t_\\t_\\tO\\n\")\n",
    "    for word, tag in zip(words, tags):\n",
    "        lines.append(f\"{word}\\t_\\t_\\t{tag}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def save_conll_format(output_str, filename):\n",
    "    \"\"\"\n",
    "    Save the given CoNLL-style string to the specified file.\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output_str)\n",
    "\n",
    "# --- Sample Input for Sub-task 1 ---\n",
    "# For sub-task 1 the input is simply the problem description (without additional XML markup).\n",
    "sample_input = (\n",
    "    \"A farmer has 100 acres to plant corn and wheat; corn yields $200/acre and uses 3 fertilizer units, wheat yields $300/acre and uses 2 fertilizer units, and only 240 fertilizer units are available—how many acres of each should be planted to maximize profit?\"\n",
    ")\n",
    "\n",
    "# Get predictions from the model.\n",
    "words, predicted_tags = predict_entities(sample_input)\n",
    "\n",
    "# Convert the predictions to CoNLL format.\n",
    "conll_output = get_conll_format(words, predicted_tags)\n",
    "print(\"Predicted Output in CoNLL Format:\\n\")\n",
    "print(conll_output)\n",
    "\n",
    "# Save the output to a file.\n",
    "output_filename = \"single_test_output-sub1.conll\"\n",
    "save_conll_format(conll_output, output_filename)\n",
    "print(f\"\\nSaved the predicted output to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262899b2-ada0-486f-ac91-3457160e48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python conll2bart_ready.py \\\n",
    "  --conll D:/LLM/NER/nl4opt-subtask1-baseline/single_test_output-sub1.conll \\\n",
    "  --out bart_inputs_single_test.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
